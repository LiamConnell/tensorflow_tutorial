{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A longwinded note on the structure of this tutorial\n",
    "\n",
    "This presentation is intended as a first look at tensorflow. It should provide enough foundation to let the student start exploring the documentation themselves. This should be reinforced by maybe 20 minutes of \"playing around\" in a notebook to solidify the concepts. \n",
    "\n",
    "Documentation of Tensorflow can be found on their website. Its very useful but, since it's a huge project, can be tough to work through. It helps to know that tensorflow is actually a collection of APIs starting with TensorFlow Core and working up to varying levels of abstraction. Some of the mid-level APIs, such as `tf.train` and `tf.nn` are commonly used even in research level. \n",
    "\n",
    "When learning tensorflow we can either start from the top or bottom (high level API or core API). Both methods have their advantages but I have chosen the latter. We will begin with the basic funcionality of the Core API and a few other APIs that help with basic mathematical operations. In later lessons we will work up to higher level APIs, but wherever possible, we will first run some basic code in Core API that does the same as the higher level function. For the student who gets bogged down by rigorous math: although occaisionally frustrating, she should keep in mind that pretty much every \"mathy bit\" will get redefined later as a one-line abstraction. \n",
    "\n",
    "This method is intended to avoid letting any part of the process become a \"black box\". Once we move on to deep learning this will be vital because it allows us to poke around inside our model and figure out if it is training correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic tensorflow intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those who have experience with numpy will be familiar with most of the functions and operations. The rest of us have experience with other forms of processing data, manipulating dataframes, matrices, and arrays in whatever language and package. \n",
    "\n",
    "Tensorflow will be the same as this, but with one more layer of required intuition: the idea of **graph based computation**. \n",
    "\n",
    "Think of it like this: instead of having objects that hold data and functions that manipulate the objects to create new objects, we have **tensors** which hold a graph of all the functions that will arrive to that point. Calling the tensor itself does nothing, instead we have to **run the function in a Session**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constants and operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.constant(4)\n",
    "y = tf.constant(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=() dtype=int32>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(x+y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants can have rank 0 or greater. Lets check out a rank two constant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = tf.constant([[1,2],[3,4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  8],\n",
       "       [12, 16]], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.multiply(x,Z)) # x * Z also works (abreviated syntax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are lots of functions for data manipulation: addition, multiplication, exponents, etc. Look at the documentation for descriptions for what they are. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  16],\n",
       "       [ 81, 256]], dtype=int32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.pow(Z, x)) # = Z^x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### placeholders\n",
    "Placeholders are empty dataframes where we input the data when we run the process. This is vital to the workflow in tensorflow. We usually use them as the \"input and output\" sets. In  `Y = M*X + b` the placeholders would be X and Y. The placeholders can hold lots of data so the model runs **batches** as we will see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(dtype = tf.float32)\n",
    "Y = tf.placeholder(dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.,  5.,  7.], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(X+Y, {X:[0,1,2], Y:[3,4,5]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "These are the **trainable** parameters of the model. They must be **initialized** to a certain value before being used.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "M = tf.Variable([.01], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01      ,  1.00999999,  2.00999999,  3.00999999], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(M + X, {X: [0,1,2,3]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After initialization, variables can be assigned to a new value. This is what \"training\" is: slowly changing the trainable variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01        1.00999999  2.00999999  3.00999999]\n",
      "[ 2.00999999  3.00999999  4.01000023  5.01000023]\n",
      "[ 4.01000023  5.01000023  6.01000023  7.01000023]\n",
      "[ 6.01000023  7.01000023  8.01000023  9.01000023]\n",
      "[  8.01000023   9.01000023  10.01000023  11.01000023]\n",
      "[ 10.01000023  11.01000023  12.01000023  13.01000023]\n",
      "[ 12.01000023  13.01000023  14.01000023  15.01000023]\n",
      "[ 14.01000023  15.01000023  16.01000023  17.01000023]\n",
      "[ 16.01000023  17.01000023  18.01000023  19.01000023]\n",
      "[ 18.01000023  19.01000023  20.01000023  21.01000023]\n"
     ]
    }
   ],
   "source": [
    "update_M = tf.assign_add(M, [2])\n",
    "for i in range(10):\n",
    "    print(sess.run(M + X, {X: [0,1,2,3]}))\n",
    "    sess.run(update_M)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is best practices to initialize a Variable using `tf.get_variable` and to assign it a name and datatype. The default initializer is random_uniform but we can change that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "my_v = tf.get_variable(\"my_v\", [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'my_v:0' shape=(1, 2, 3) dtype=float32_ref>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dont forget to initialize!\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.5787527 , -0.34723955,  0.8445853 ],\n",
       "        [-0.38655049, -1.0259459 , -0.76001722]]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(my_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time to make a linear regression, boludo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "x_train = [0.0,1.0,2.0,3.0]\n",
    "y_train = [4.0,5.0,6.0,7.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1 MODEL\n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None,])\n",
    "y = tf.placeholder(dtype = tf.float32, shape = [None,])\n",
    "M = tf.get_variable(\"M\", dtype = tf.float32, initializer=tf.constant([0.1]))\n",
    "b = tf.get_variable(\"b\", dtype = tf.float32, initializer=tf.constant([0.1]))\n",
    "linear_model = M * x + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.19999981,  4.29999971], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "sess.run(tf.assign_add(M,[2]))\n",
    "sess.run(linear_model, {x: [1,2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2 LOSS\n",
    "sq_error = (y - linear_model)**2\n",
    "loss = tf.reduce_mean(sq_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 3 OPTIMIZER\n",
    "## this is the CANCHERO VERSION OF GRADIENT_DESCENT OPTIMIZER in order to show the math... the more commonly used one is below, \n",
    "## but I wanted to show the functionality of the basic API. Most tutorials dont even bother showing this becuase\n",
    "## its not practical\n",
    "step = .01 # HYPERPARAM ALERT!!! also note: this will often be even smaller\n",
    "g = tf.gradients(loss, [M,b]) # computes the gradient (vector of partial derivatives) of the loss function w.r.t. the variables M and b. Computed through the magic of back propagation\n",
    "assign_M = tf.assign_sub(M, g[0]*[step]) # subtracts (dL/dM) * step) from M and assigns that as the new M\n",
    "assign_b = tf.assign_sub(b, g[1]*[step]) # subtracts (dL/db) * step) from b and assigns that as the new b\n",
    "def train_canchero():\n",
    "    return assign_M, assign_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# iterate through the training step \n",
    "for i in range(1000):\n",
    "    sess.run(train_canchero(), {x: x_train, y: y_train})\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00342476] [ 3.99268842]\n"
     ]
    }
   ],
   "source": [
    "print(sess.run(M), sess.run(b)) # thats pretty close to what we'd expect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will probably always use the **train** api instead of the canchero version. Its much easier and not error prone. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 3 OPTIMIZER\n",
    "## Essentially all this does is (1) get the gradient of the loss function wrt variables (2) step the variable accordingly\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01) # step param\n",
    "train = optimizer.minimize(loss) # note: this automatically runs it on ALL variables, that's why we dont have to tell it which ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.0000093] [ 3.99998021]\n"
     ]
    }
   ],
   "source": [
    "# init again and run\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(10000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n",
    "print(sess.run(M), sess.run(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I marked the three essencial modules that any Machine Learning problem will have: Model, Loss, Optimizer. These three modules can be interchanged with other techniques according to our needs. Where an LSTM would fit in? Cross Entropy? Adagrad? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A look at what's ahead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same dumb training data, we will train a neural network. Notice that, since our net also goes from one dimension to one dimension, we use the same loss function (MSE) to compare the model's output with the target data. Since the gradient_descent_optimizer automatically looks at ALL variables, we dont even have to worry about naming it all the variables; the optimization module looks the same as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# training data (we need to format it differently so that it is accepted in training)\n",
    "x_train = np.array([0.0,1.0,2.0,3.0])\n",
    "y_train = np.array([4.0,5.0,6.0,7.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model\n",
    "n_input = 1 # input dimension\n",
    "n_hidden_1 = 10 # 1st layer number of features\n",
    "n_hidden_2 = 10 # 2nd layer number of features\n",
    "n_out = 1 # out dim\n",
    "\n",
    "# define placeholders \n",
    "x = tf.placeholder(tf.float32, [None, ])\n",
    "y = tf.placeholder(tf.float32,  [None, ])\n",
    "\n",
    "# define variables\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_out]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_out]))\n",
    "}\n",
    "\n",
    "# define model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    x = tf.reshape(x, [4,1])\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)  # ReLU non-linearity stops linear functions from collapsing into a single linear function!\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    out_layer = tf.reshape(out_layer, [4])\n",
    "    return out_layer\n",
    "\n",
    "# Construct model\n",
    "model = multilayer_perceptron(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "sq_error = (y - model)**2\n",
    "loss = tf.reduce_mean(sq_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "for i in range(10000):\n",
    "    sess.run(train, {x: x_train, y: y_train})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x11c4e6c88>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXkAAAEACAYAAABWLgY0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11c330a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(sess.run(model, {x: x_train, y: y_train})) # <3 <3"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
