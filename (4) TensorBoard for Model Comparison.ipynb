{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_datareader.data as web\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime\n",
    "import functools\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We abandon the idea of inheritance in favor of a universal \"Model\" class that contains all functionality that we have used so far. The model is defined layer by layer in a hyperparameter dictionary.\n",
    "\n",
    "We also begin to use the TensorBoard to visualize our graph and any summary statistics that we choose to log. We separate this logging into distinct directories so that we can compare separate experiments in tensorboard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            with tf.variable_scope(function.__name__):\n",
    "                setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_relu(input, output_dim, relu=True):\n",
    "    input_dim = input.get_shape().as_list()[1]\n",
    "    weights = tf.get_variable(\"weights\",shape=[input_dim, output_dim])\n",
    "    biases = tf.get_variable(\"biases\", shape=[output_dim])\n",
    "    out = tf.add(tf.matmul(input, weights), biases)\n",
    "    if relu:\n",
    "        out = tf.nn.relu(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_relu(input, kernel_shape, bias_shape):\n",
    "    weights = tf.get_variable(\"weights\", kernel_shape,\n",
    "        initializer=tf.random_normal_initializer())\n",
    "    biases = tf.get_variable(\"biases\", bias_shape,\n",
    "        initializer=tf.constant_initializer(0.0))\n",
    "    conv = tf.nn.conv2d(input, weights,\n",
    "        strides=[1, 1, 1, 1], padding='SAME')\n",
    "    return tf.nn.relu(conv + biases)\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, input_data, target_data, hparams, keep_prob): \n",
    "    \n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        self.in_dim, self.out_dim = self.get_inout_dim()\n",
    "        \n",
    "        self.keep_prob = keep_prob\n",
    "        self.predict_builder = hparams.PREDICT_BUILDER\n",
    "        self.learning_rate = hparams.LEARNING_RATE\n",
    "        \n",
    "\n",
    "        self.prediction \n",
    "        self.loss \n",
    "        self.train \n",
    "        self.evaluate\n",
    "        \n",
    "        self.merged = tf.summary.merge_all()\n",
    "\n",
    "        \n",
    "    def get_inout_dim(self):\n",
    "        in_dim = self.input_data.get_shape().as_list()[1]\n",
    "        out_dim = self.target_data.get_shape().as_list()[1]\n",
    "        return in_dim, out_dim\n",
    "    \n",
    "    @define_scope\n",
    "    def prediction(self):\n",
    "        \"\"\"Builds a graph of layers based on hparams.PREDICT_BUILDER.\n",
    "        \n",
    "        Types of layers:\n",
    "            * reshape(a,b,c...) calls tf.reshape and passes dims\n",
    "            * fconn_relu(a) calls fully_connected_relu with dim\n",
    "            * fconn calls(a) fully_connected_relu with dim and relu=False\n",
    "            * conv_maxp(a,b,c,d,e) calls conv_relu (a single-stepping, same-padded conv layer)\n",
    "                        - conv is a*b pixels\n",
    "                        - pixels have c colors\n",
    "                        - conv outputs d colors\n",
    "                        - bias is shape e (I think always the same as d?)\n",
    "                + then calls max_pool_2x2 which cuts the pixel side length in half (no dims)\n",
    "            \n",
    "        \n",
    "        dims special cases:\n",
    "            * dims=='out': dims <- self.out_dim    (so far a scalar that works with fconn)\n",
    "            * len(dims)==1: dims <- dims[0]    (so that can be passed into fconn[_relu])\n",
    "        \n",
    "        \"\"\"\n",
    "        x = self.input_data\n",
    "        \n",
    "        count_dict = {}\n",
    "        for builder in self.predict_builder:\n",
    "            layer, dims = builder.split('|')\n",
    "            if dims == 'out': \n",
    "                dims = self.out_dim\n",
    "            else: \n",
    "                dims = [int(i) for i in dims.split(',')] \n",
    "                if len(dims)==1:\n",
    "                    dims = dims[0]\n",
    "            \n",
    "            if layer == 'reshape':\n",
    "                try: count_dict[layer]+=1\n",
    "                except: count_dict[layer]=1\n",
    "                with tf.variable_scope(''.join([layer, str(count_dict[layer])])):\n",
    "                    x = tf.reshape(x, dims)\n",
    "            \n",
    "            if layer == 'fconn_relu':\n",
    "                try: count_dict[layer]+=1\n",
    "                except: count_dict[layer]=1\n",
    "                with tf.variable_scope(''.join([layer, str(count_dict[layer])])):\n",
    "                    x = fully_connected_relu(x, dims)\n",
    "                    \n",
    "            if layer == 'fconn':\n",
    "                try: count_dict[layer]+=1\n",
    "                except: count_dict[layer]=1\n",
    "                with tf.variable_scope(''.join([layer, str(count_dict[layer])])):\n",
    "                    x = fully_connected_relu(x, dims, relu=False)\n",
    "                    \n",
    "            if layer == 'conv_maxp':\n",
    "                try: count_dict[layer]+=1\n",
    "                except: count_dict[layer]=1\n",
    "                with tf.variable_scope(''.join([layer, str(count_dict[layer])])):\n",
    "                    x = conv_relu(x, dims[0:-1], [dims[-1]])  \n",
    "                    x = max_pool_2x2(x) \n",
    "                    \n",
    "            if layer == 'dropout':\n",
    "                try: count_dict[layer]+=1\n",
    "                except: count_dict[layer]=1\n",
    "                with tf.variable_scope(''.join([layer, str(count_dict[layer])])):\n",
    "                    x = tf.nn.dropout(x, self.keep_prob)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    @define_scope\n",
    "    def loss(self):\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels = self.target_data, logits = self.prediction))\n",
    "        tf.summary.scalar('loss', loss) ##summary##\n",
    "        return loss\n",
    "    \n",
    "    @define_scope\n",
    "    def train(self):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "        train = optimizer.minimize(self.loss)\n",
    "        return train\n",
    "    \n",
    "    @define_scope\n",
    "    def evaluate(self):\n",
    "        dense = tf.argmax(Y, axis=1)\n",
    "        dense2 = tf.argmax(self.prediction, axis=1)\n",
    "        correct = tf.equal(dense, dense2)\n",
    "        score = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "        tf.summary.scalar('score', score) ##summary##\n",
    "        return score\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = {\n",
    "    'logdir':'./logdir/'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HParam model defs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# simple NN\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    LOGDIR = FLAGS['logdir'] + 'simpleNN/' + str(datetime.datetime.now().strftime('%Y%m%d_%H%M%S')),\n",
    "    LEARNING_RATE=0.001,\n",
    "    PREDICT_BUILDER = [\n",
    "        'fconn_relu|10', \n",
    "        'fconn_relu|10', \n",
    "        'fconn|out'\n",
    "    ],\n",
    "    KEEP_PROB = .75\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convolutional NN\n",
    "hparams = tf.contrib.training.HParams(\n",
    "    LOGDIR = FLAGS['logdir'] + 'convnet/' + str(datetime.datetime.now().strftime('%Y%m%d_%H%M%S')),\n",
    "    LEARNING_RATE=0.001,\n",
    "    PREDICT_BUILDER = [\n",
    "        'reshape|-1,28,28,1', \n",
    "        'conv_maxp|5,5,1,32,32', \n",
    "        'conv_maxp|5,5,32,64,64',\n",
    "        'reshape|-1,3136',\n",
    "        'fconn_relu|1024',\n",
    "        'dropout|0',\n",
    "        'fconn|out'\n",
    "    ],\n",
    "    KEEP_PROB = .75\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Creation and Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create the graph and initialize it in a session\n",
    "tf.reset_default_graph() \n",
    "X = tf.placeholder(dtype=tf.float32, shape = [None, 784])\n",
    "Y = tf.placeholder(dtype=tf.float64, shape = [None, 10])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "model = Model(X, Y, hparams, keep_prob)\n",
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.summary.FileWriter(hparams.LOGDIR, graph=tf.get_default_graph())#sess.graph)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy   8.97% \t\tX-Ent Loss:  67.83\n",
      "Test accuracy  58.41% \t\tX-Ent Loss:   1.34\n",
      "Test accuracy  73.80% \t\tX-Ent Loss:   0.84\n",
      "Test accuracy  77.49% \t\tX-Ent Loss:   0.71\n",
      "Test accuracy  81.05% \t\tX-Ent Loss:   0.60\n",
      "Test accuracy  80.31% \t\tX-Ent Loss:   0.61\n",
      "Test accuracy  84.45% \t\tX-Ent Loss:   0.51\n",
      "Test accuracy  85.95% \t\tX-Ent Loss:   0.46\n",
      "Test accuracy  86.04% \t\tX-Ent Loss:   0.45\n",
      "Test accuracy  86.71% \t\tX-Ent Loss:   0.45\n"
     ]
    }
   ],
   "source": [
    "# iterate through evaluations\n",
    "iter = 0\n",
    "for i in range(10):\n",
    "    images, labels = mnist.test.images, mnist.test.labels\n",
    "    loss, evaluate, summary = sess.run([model.loss, model.evaluate, model.merged], feed_dict={X: images, Y: labels, keep_prob: 1.0})\n",
    "    writer.add_summary(summary, iter)\n",
    "    print('Test accuracy {:6.2f}% \\t\\tX-Ent Loss: {:6.2f}'.format(100 * evaluate, loss))\n",
    "    \n",
    "    # nested iterate through training steps\n",
    "    for j in range(10):\n",
    "        data = mnist.train.next_batch(100)\n",
    "        images, labels = data[0], data[1]\n",
    "        _, summary = sess.run([model.train, model.merged] , {X: data[0], Y: data[1], keep_prob: hparams.KEEP_PROB})\n",
    "        writer.add_summary(summary, iter)\n",
    "        \n",
    "        iter +=1\n",
    "        \n",
    "summary = sess.run(model.merged, feed_dict={X: images, Y: labels, keep_prob: 1.0})\n",
    "writer.add_summary(summary, iter)\n",
    "writer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "* LSTM (new dataset)\n",
    "* feeddict as function\n",
    "* optimizer hparams\n",
    "* hparams as json to logs dirs\n",
    "* read logs file and hparams.json into python\n",
    "* epochs and iters as hparams\n",
    "* other best practices:\n",
    "    * vars init set to best level\n",
    "    * step set inteligently\n",
    "* random hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
